import os
from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.docstore.document import Document

from config import get_settings

settings = get_settings()

async def index_knowledge_base():
    """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å Knowledge Base –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    
    print("üîÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è Knowledge Base...")
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
    kb_path = "./data/knowledge_base/islamic_finance_kb.md"
    
    if not os.path.exists(kb_path):
        print("‚ö†Ô∏è  Knowledge Base –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    with open(kb_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # –†–∞–∑–±–∏—Ç—å –Ω–∞ chunks –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
    chunks = split_by_sections(content)
    
    # –°–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = []
    for i, chunk in enumerate(chunks):
        metadata = extract_metadata(chunk)
        metadata["chunk_id"] = i
        
        doc = Document(
            page_content=chunk,
            metadata=metadata
        )
        documents.append(doc)
    
    # –°–æ–∑–¥–∞—Ç—å embeddings
    embeddings = OpenAIEmbeddings(
        model=settings.embedding_model,
        openai_api_key=settings.openai_api_key,
        openai_api_base=settings.openai_base_url
    )
    
    # –°–æ–∑–¥–∞—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name=settings.chroma_collection,
        persist_directory="./data/embeddings/chroma_db"
    )
    
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(documents)} chunks")
    
    return vectorstore

def split_by_sections(content: str) -> List[str]:
    """–†–∞–∑–±–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º"""
    sections = []
    current_section = []
    
    for line in content.split("\n"):
        if line.startswith("## –†–ê–ó–î–ï–õ"):
            if current_section:
                sections.append("\n".join(current_section))
            current_section = [line]
        else:
            current_section.append(line)
    
    if current_section:
        sections.append("\n".join(current_section))
    
    return sections

def extract_metadata(chunk: str) -> dict:
    """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ chunk"""
    metadata = {}
    
    # –ò–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞
    if "## –†–ê–ó–î–ï–õ" in chunk:
        section_line = chunk.split("\n")[0]
        metadata["section"] = section_line.strip()
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if "–¢–ï–†–ú–ò–ù" in section_line or "–ì–õ–û–°–°–ê–†–ò–ô" in section_line:
            metadata["type"] = "term"
        elif "FAQ" in section_line or "–í–û–ü–†–û–°" in section_line:
            metadata["type"] = "faq"
        elif "–ü–†–û–î–£–ö–¢" in section_line:
            metadata["type"] = "product"
        else:
            metadata["type"] = "concept"
    
    return metadata